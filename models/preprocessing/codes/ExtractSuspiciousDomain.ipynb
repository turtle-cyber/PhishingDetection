{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "452d6c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: C:\\Users\\Turtleneck-4\\Desktop\\Phishing_Detection\\models\\model_training\\PhishingUrlData\\Mock_Data_01_08_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_01_08_2025.xlsx:   0%|          | 0/1244 [00:00<?, ?it/s]C:\\Users\\Turtleneck-4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py:427: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  return futures.Future(loop=self)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "  Checking Mock_Data_01_08_2025.xlsx: 100%|██████████| 1244/1244 [01:20<00:00, 15.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote 402 active rows to sheet 'Active_Domains' in Mock_Data_01_08_2025.xlsx\n",
      "Processing: C:\\Users\\Turtleneck-4\\Desktop\\Phishing_Detection\\models\\model_training\\PhishingUrlData\\Mock_Data_03_09_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_03_09_2025.xlsx: 100%|██████████| 1372/1372 [00:42<00:00, 32.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote 1333 active rows to sheet 'Active_Domains' in Mock_Data_03_09_2025.xlsx\n",
      "Processing: C:\\Users\\Turtleneck-4\\Desktop\\Phishing_Detection\\models\\model_training\\PhishingUrlData\\Mock_Data_04_08_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_04_08_2025.xlsx: 100%|██████████| 29/29 [00:05<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote 5 active rows to sheet 'Active_Domains' in Mock_Data_04_08_2025.xlsx\n",
      "Processing: C:\\Users\\Turtleneck-4\\Desktop\\Phishing_Detection\\models\\model_training\\PhishingUrlData\\Mock_Data_05_08_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_05_08_2025.xlsx: 100%|██████████| 1/1 [00:00<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  No active rows found in Mock_Data_05_08_2025.xlsx; no sheet written.\n",
      "Processing: C:\\Users\\Turtleneck-4\\Desktop\\Phishing_Detection\\models\\model_training\\PhishingUrlData\\Mock_Data_07_08_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_07_08_2025.xlsx: 100%|██████████| 6/6 [00:11<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote 6 active rows to sheet 'Active_Domains' in Mock_Data_07_08_2025.xlsx\n",
      "Processing: C:\\Users\\Turtleneck-4\\Desktop\\Phishing_Detection\\models\\model_training\\PhishingUrlData\\Mock_Data_12_08_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_12_08_2025.xlsx: 100%|██████████| 7/7 [00:11<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  No active rows found in Mock_Data_12_08_2025.xlsx; no sheet written.\n",
      "Processing: C:\\Users\\Turtleneck-4\\Desktop\\Phishing_Detection\\models\\model_training\\PhishingUrlData\\Mock_Data_12_09_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_12_09_2025.xlsx: 100%|██████████| 277/277 [00:23<00:00, 11.89it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote 32 active rows to sheet 'Active_Domains' in Mock_Data_12_09_2025.xlsx\n",
      "Processing: C:\\Users\\Turtleneck-4\\Desktop\\Phishing_Detection\\models\\model_training\\PhishingUrlData\\Mock_Data_14_08_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_14_08_2025.xlsx: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote 1 active rows to sheet 'Active_Domains' in Mock_Data_14_08_2025.xlsx\n",
      "Processing: C:\\Users\\Turtleneck-4\\Desktop\\Phishing_Detection\\models\\model_training\\PhishingUrlData\\Mock_Data_19_08_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_19_08_2025.xlsx: 100%|██████████| 15/15 [00:05<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote 4 active rows to sheet 'Active_Domains' in Mock_Data_19_08_2025.xlsx\n",
      "Processing: C:\\Users\\Turtleneck-4\\Desktop\\Phishing_Detection\\models\\model_training\\PhishingUrlData\\Mock_Data_21_08_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_21_08_2025.xlsx: 100%|██████████| 4/4 [00:02<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  No active rows found in Mock_Data_21_08_2025.xlsx; no sheet written.\n",
      "Processing: C:\\Users\\Turtleneck-4\\Desktop\\Phishing_Detection\\models\\model_training\\PhishingUrlData\\Mock_Data_22_08_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_22_08_2025.xlsx: 100%|██████████| 6/6 [00:00<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote 4 active rows to sheet 'Active_Domains' in Mock_Data_22_08_2025.xlsx\n",
      "Processing: C:\\Users\\Turtleneck-4\\Desktop\\Phishing_Detection\\models\\model_training\\PhishingUrlData\\Mock_Data_25_08_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_25_08_2025.xlsx: 100%|██████████| 7/7 [00:11<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  No active rows found in Mock_Data_25_08_2025.xlsx; no sheet written.\n",
      "Processing: C:\\Users\\Turtleneck-4\\Desktop\\Phishing_Detection\\models\\model_training\\PhishingUrlData\\Mock_Data_28_08_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_28_08_2025.xlsx: 100%|██████████| 4/4 [00:01<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  No active rows found in Mock_Data_28_08_2025.xlsx; no sheet written.\n",
      "Processing: C:\\Users\\Turtleneck-4\\Desktop\\Phishing_Detection\\models\\model_training\\PhishingUrlData\\Mock_Data_29_08_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_29_08_2025.xlsx: 100%|██████████| 12/12 [00:00<00:00, 21.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  No active rows found in Mock_Data_29_08_2025.xlsx; no sheet written.\n",
      "Processing: C:\\Users\\Turtleneck-4\\Desktop\\Phishing_Detection\\models\\model_training\\PhishingUrlData\\Mock_Data_31_07_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_31_07_2025.xlsx: 100%|██████████| 302/302 [00:23<00:00, 12.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote 21 active rows to sheet 'Active_Domains' in Mock_Data_31_07_2025.xlsx\n",
      "Summary written to checker_summary.csv\n",
      "✅ Saved 1808 active domains to ActiveDomains_Final.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import aiodns\n",
    "import pandas as pd\n",
    "import socket\n",
    "import os\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import ssl\n",
    "\n",
    "# CONFIG\n",
    "FOLDER = Path(\"C:\\\\Users\\\\Turtleneck-4\\\\Desktop\\\\Phishing_Detection\\\\models\\\\model_training\\\\PhishingUrlData\")   # folder with Excel files\n",
    "EXCEL_GLOB = \"*.xls*\"\n",
    "SHEET_NAME_OUT = \"Active_Domains\"\n",
    "POSSIBLE_COLUMN_NAMES = [\n",
    "    \"Identified Phishing/Suspected Domain Name\",\n",
    "    \"Identified Phishing/Suspected Domain\",\n",
    "    \"Domain\",\n",
    "    \"URL\",\n",
    "    \"Identified Phishing/Suspected Domain Name \",\n",
    "    \"Identified Phishing/Suspected Domain Name  \",\n",
    "    \"Phishing/Suspected Domain Name\",\n",
    "    \"Identified Phishing/Suspected Domain\",\n",
    "    \"Identified Phishing/Suspected Domain Name\"\n",
    "]\n",
    "MAX_CONCURRENT = 40            # concurrency for HTTP checks (reduce if hitting issues)\n",
    "REQUEST_TIMEOUT = 10          # seconds for HTTP request timeout\n",
    "DNS_TIMEOUT = 5               # seconds for DNS lookup\n",
    "FOLLOW_REDIRECTS = True\n",
    "VERIFY_SSL = True             # set False to accept invalid SSL/TLS (not recommended)\n",
    "SUMMARY_CSV = \"checker_summary.csv\"\n",
    "\n",
    "# Helper to normalise extracted candidate\n",
    "def normalize_value(v):\n",
    "    if pd.isna(v):\n",
    "        return None\n",
    "    s = str(v).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    return s\n",
    "\n",
    "def guess_url_candidates_from_cell(cell_text):\n",
    "    # return a list of url candidates in that cell (usually one)\n",
    "    s = cell_text.strip()\n",
    "    # if it looks like just a domain (no scheme), return as-is\n",
    "    return [s]\n",
    "\n",
    "# DNS resolve (uses aiodns)\n",
    "async def resolve_domain(resolver, domain):\n",
    "    try:\n",
    "        # try A record\n",
    "        ans = await resolver.gethostbyname(domain, socket.AF_INET)\n",
    "        if ans and ans.addresses:\n",
    "            return True, ans.addresses\n",
    "        return False, []\n",
    "    except Exception:\n",
    "        return False, []\n",
    "\n",
    "# Try HTTP(S) request attempts\n",
    "async def try_http(session, candidate):\n",
    "    \"\"\"\n",
    "    Try the candidate as-is and with http/https prefixes. Returns (status_ok, details)\n",
    "    details: dict with keys: 'final_url','status','error'\n",
    "    \"\"\"\n",
    "    tried = []\n",
    "    to_try = []\n",
    "    parsed = urlparse(candidate)\n",
    "    if parsed.scheme:\n",
    "        to_try.append(candidate)\n",
    "    else:\n",
    "        # try as-is (may be domain-only) then http and https\n",
    "        to_try.append(candidate)\n",
    "        to_try.append(\"https://\" + candidate)\n",
    "        to_try.append(\"http://\" + candidate)\n",
    "\n",
    "    for url in to_try:\n",
    "        if url in tried:\n",
    "            continue\n",
    "        tried.append(url)\n",
    "        try:\n",
    "            # Make a HEAD first to be light, fallback to GET if server rejects HEAD\n",
    "            timeout = aiohttp.ClientTimeout(total=REQUEST_TIMEOUT)\n",
    "            async with session.head(url, allow_redirects=FOLLOW_REDIRECTS, timeout=timeout) as resp:\n",
    "                status = resp.status\n",
    "                final = str(resp.url)\n",
    "                if 200 <= status < 400:\n",
    "                    return True, {\"final_url\": final, \"status\": status, \"tried\": url}\n",
    "                # treat some 4xx/5xx as not active\n",
    "                # continue trying other schemes\n",
    "        except aiohttp.ClientResponseError as e:\n",
    "            # maybe HEAD not allowed, try GET\n",
    "            try:\n",
    "                timeout = aiohttp.ClientTimeout(total=REQUEST_TIMEOUT)\n",
    "                async with session.get(url, allow_redirects=FOLLOW_REDIRECTS, timeout=timeout) as resp2:\n",
    "                    status = resp2.status\n",
    "                    final = str(resp2.url)\n",
    "                    if 200 <= status < 400:\n",
    "                        return True, {\"final_url\": final, \"status\": status, \"tried\": url}\n",
    "            except Exception:\n",
    "                pass\n",
    "        except Exception:\n",
    "            # network error, continue to next variant\n",
    "            pass\n",
    "    return False, {\"error\": \"all attempts failed\", \"tried\": tried}\n",
    "\n",
    "# Main check per cell/value\n",
    "async def check_candidate(session, resolver, raw_value, semaphore):\n",
    "    \"\"\"\n",
    "    returns dict:\n",
    "      {\n",
    "        'raw_value': raw_value,\n",
    "        'normalized': normalized_value,\n",
    "        'dns_resolved': True/False,\n",
    "        'http_active': True/False,\n",
    "        'http_details': {...},\n",
    "        'active': True/False\n",
    "      }\n",
    "    \"\"\"\n",
    "    out = {\"raw_value\": raw_value}\n",
    "    val = normalize_value(raw_value)\n",
    "    out['normalized'] = val\n",
    "    if not val:\n",
    "        out.update({'dns_resolved': False, 'http_active': False, 'http_details': None, 'active': False})\n",
    "        return out\n",
    "\n",
    "    # extract domain part for DNS - if it's a URL, parse\n",
    "    parsed = urlparse(val if \"://\" in val else (\"http://\" + val))\n",
    "    domain = parsed.hostname\n",
    "\n",
    "    dns_ok = False\n",
    "    dns_addrs = []\n",
    "    if domain:\n",
    "        try:\n",
    "            # DNS resolve\n",
    "            dns_ok, dns_addrs = await resolve_domain(resolver, domain)\n",
    "        except Exception:\n",
    "            dns_ok = False\n",
    "\n",
    "    out['dns_resolved'] = dns_ok\n",
    "    out['dns_addresses'] = dns_addrs\n",
    "\n",
    "    # HTTP check (preferred indicator)\n",
    "    # Use semaphore to limit concurrency\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            http_ok, http_details = await try_http(session, val)\n",
    "        except Exception as e:\n",
    "            http_ok = False\n",
    "            http_details = {\"error\": str(e)}\n",
    "\n",
    "    out['http_active'] = http_ok\n",
    "    out['http_details'] = http_details\n",
    "\n",
    "    # decide active: prefer http_ok; if http_ok False but DNS ok treat as active=maybe (parked)\n",
    "    if http_ok:\n",
    "        out['active'] = True\n",
    "    else:\n",
    "        # if DNS resolves but no HTTP, it could be a parked domain or non-web host; treat as inactive for web-case\n",
    "        out['active'] = False\n",
    "\n",
    "    return out\n",
    "\n",
    "async def process_file(path: Path, summary_rows):\n",
    "    print(f\"Processing: {path}\")\n",
    "    # read Excel file\n",
    "    try:\n",
    "        xls = pd.ExcelFile(path)\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR reading {path}: {e}\")\n",
    "        return\n",
    "\n",
    "    # For simplicity, use first sheet to find domain column\n",
    "    df_all = pd.read_excel(path, sheet_name=0, engine=\"openpyxl\")\n",
    "    # find column containing domains\n",
    "    domain_col = None\n",
    "    for cname in POSSIBLE_COLUMN_NAMES:\n",
    "        if cname in df_all.columns:\n",
    "            domain_col = cname\n",
    "            break\n",
    "    if domain_col is None:\n",
    "        # fallback: find first column whose dtype is object and contains something with dot\n",
    "        for col in df_all.columns:\n",
    "            sample = df_all[col].dropna().astype(str).head(50).tolist()\n",
    "            if any(\".\" in s for s in sample):\n",
    "                domain_col = col\n",
    "                break\n",
    "    if domain_col is None:\n",
    "        # give up for this file\n",
    "        print(f\"  Could not auto-detect domain/URL column in {path.name}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    print(f\"  Using column: {domain_col}\")\n",
    "\n",
    "    # prepare aiohttp client + dns resolver\n",
    "    conn = aiohttp.TCPConnector(ssl=ssl.create_default_context() if VERIFY_SSL else False, limit_per_host=MAX_CONCURRENT)\n",
    "    resolver = aiodns.DNSResolver(timeout=DNS_TIMEOUT)\n",
    "    timeout = aiohttp.ClientTimeout(total=REQUEST_TIMEOUT)\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT)\n",
    "\n",
    "    async with aiohttp.ClientSession(connector=conn, timeout=timeout) as session:\n",
    "        tasks = []\n",
    "        rows = []\n",
    "        for idx, row in df_all.iterrows():\n",
    "            rawval = row.get(domain_col, None)\n",
    "            tasks.append(check_candidate(session, resolver, rawval, semaphore))\n",
    "            rows.append((idx, row))  # keep row index and row\n",
    "\n",
    "        results = []\n",
    "        # tqdm wrapper for async tasks\n",
    "        for coro in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=f\"  Checking {path.name}\"):\n",
    "            res = await coro\n",
    "            results.append(res)\n",
    "\n",
    "    # Map results back to rows order (as they came)\n",
    "    active_indices = []\n",
    "    active_records = []\n",
    "    for (idx, row), res in zip(rows, results):\n",
    "        active_flag = res.get('active', False)\n",
    "        if active_flag:\n",
    "            # combine row dict with check details\n",
    "            rdict = row.to_dict()\n",
    "            rdict['_checked_normalized'] = res.get('normalized')\n",
    "            rdict['_dns_resolved'] = res.get('dns_resolved')\n",
    "            rdict['_dns_addresses'] = \", \".join(res.get('dns_addresses') or [])\n",
    "            rdict['_http_active'] = res.get('http_active')\n",
    "            rdict['_http_details'] = str(res.get('http_details'))\n",
    "            active_records.append(rdict)\n",
    "            active_indices.append(idx)\n",
    "\n",
    "        # summary row add\n",
    "        summary_rows.append({\n",
    "            \"filename\": path.name,\n",
    "            \"row_index\": idx,\n",
    "            \"raw_value\": res.get('raw_value'),\n",
    "            \"normalized\": res.get('normalized'),\n",
    "            \"dns_resolved\": res.get('dns_resolved'),\n",
    "            \"http_active\": res.get('http_active'),\n",
    "            \"http_details\": str(res.get('http_details')),\n",
    "            \"active\": res.get('active')\n",
    "        })\n",
    "\n",
    "    # write active records to new sheet in same workbook\n",
    "    if active_records:\n",
    "        out_df = pd.DataFrame(active_records)\n",
    "        try:\n",
    "            # open existing workbook, append sheet\n",
    "            with pd.ExcelWriter(path, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "                out_df.to_excel(writer, sheet_name=SHEET_NAME_OUT, index=False)\n",
    "            print(f\"  Wrote {len(active_records)} active rows to sheet '{SHEET_NAME_OUT}' in {path.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR writing sheet to {path.name}: {e}\")\n",
    "    else:\n",
    "        print(f\"  No active rows found in {path.name}; no sheet written.\")\n",
    "\n",
    "async def main():\n",
    "    folder = Path(FOLDER)\n",
    "    files = list(folder.glob(EXCEL_GLOB))\n",
    "    if not files:\n",
    "        print(f\"No Excel files found in {folder.resolve()}\")\n",
    "        return\n",
    "\n",
    "    summary_rows = []\n",
    "    # process each file sequentially (to avoid too many concurrent connections across files)\n",
    "    for f in files:\n",
    "        try:\n",
    "            await process_file(f, summary_rows)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {f.name}: {e}\")\n",
    "\n",
    "    # write summary csv\n",
    "    if summary_rows:\n",
    "        keys = [\"filename\",\"row_index\",\"raw_value\",\"normalized\",\"dns_resolved\",\"http_active\",\"http_details\",\"active\"]\n",
    "        with open(SUMMARY_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as csvf:\n",
    "            writer = csv.DictWriter(csvf, fieldnames=keys)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(summary_rows)\n",
    "        print(f\"Summary written to {SUMMARY_CSV}\")\n",
    "        df_summary = pd.DataFrame(summary_rows)\n",
    "        active_df = df_summary[df_summary[\"active\"] == True]\n",
    "\n",
    "        if not active_df.empty:\n",
    "            active_file = \"ActiveDomains_Final.xlsx\"\n",
    "            active_df.to_excel(active_file, index=False)\n",
    "            print(f\"✅ Saved {len(active_df)} active domains to {active_file}\")\n",
    "        else:\n",
    "            print(\"⚠️ No active domains found across all files.\")\n",
    "    \n",
    "\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7e77bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phishing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
