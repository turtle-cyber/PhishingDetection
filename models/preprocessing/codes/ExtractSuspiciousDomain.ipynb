{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38770f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: C:\\Users\\HP\\Desktop\\PhishingDetection\\models\\model_training\\PhishingUrlData\\Mock_Data_01_08_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_01_08_2025.xlsx: 100%|██████████| 1244/1244 [02:18<00:00,  8.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Mock_Data_01_08_2025.xlsx: [Errno 13] Permission denied: 'C:\\\\Users\\\\HP\\\\Desktop\\\\PhishingDetection\\\\models\\\\model_training\\\\PhishingUrlData\\\\Mock_Data_01_08_2025.xlsx'\n",
      "Processing: C:\\Users\\HP\\Desktop\\PhishingDetection\\models\\model_training\\PhishingUrlData\\Mock_Data_03_09_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_03_09_2025.xlsx: 100%|██████████| 1372/1372 [01:02<00:00, 22.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote 1327 active rows to sheet 'Active_Domains' in Mock_Data_03_09_2025.xlsx\n",
      "Processing: C:\\Users\\HP\\Desktop\\PhishingDetection\\models\\model_training\\PhishingUrlData\\Mock_Data_04_08_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_04_08_2025.xlsx: 100%|██████████| 29/29 [00:08<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote 5 active rows to sheet 'Active_Domains' in Mock_Data_04_08_2025.xlsx\n",
      "Processing: C:\\Users\\HP\\Desktop\\PhishingDetection\\models\\model_training\\PhishingUrlData\\Mock_Data_05_08_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_05_08_2025.xlsx: 100%|██████████| 1/1 [00:00<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No active rows found in Mock_Data_05_08_2025.xlsx\n",
      "Processing: C:\\Users\\HP\\Desktop\\PhishingDetection\\models\\model_training\\PhishingUrlData\\Mock_Data_07_08_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_07_08_2025.xlsx: 100%|██████████| 6/6 [00:23<00:00,  3.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote 6 active rows to sheet 'Active_Domains' in Mock_Data_07_08_2025.xlsx\n",
      "Processing: C:\\Users\\HP\\Desktop\\PhishingDetection\\models\\model_training\\PhishingUrlData\\Mock_Data_12_08_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_12_08_2025.xlsx: 100%|██████████| 7/7 [00:45<00:00,  6.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No active rows found in Mock_Data_12_08_2025.xlsx\n",
      "Processing: C:\\Users\\HP\\Desktop\\PhishingDetection\\models\\model_training\\PhishingUrlData\\Mock_Data_12_09_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_12_09_2025.xlsx: 100%|██████████| 277/277 [00:50<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote 30 active rows to sheet 'Active_Domains' in Mock_Data_12_09_2025.xlsx\n",
      "Processing: C:\\Users\\HP\\Desktop\\PhishingDetection\\models\\model_training\\PhishingUrlData\\Mock_Data_14_08_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_14_08_2025.xlsx: 100%|██████████| 1/1 [00:03<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No active rows found in Mock_Data_14_08_2025.xlsx\n",
      "Processing: C:\\Users\\HP\\Desktop\\PhishingDetection\\models\\model_training\\PhishingUrlData\\Mock_Data_19_08_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_19_08_2025.xlsx: 100%|██████████| 15/15 [00:17<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote 4 active rows to sheet 'Active_Domains' in Mock_Data_19_08_2025.xlsx\n",
      "Processing: C:\\Users\\HP\\Desktop\\PhishingDetection\\models\\model_training\\PhishingUrlData\\Mock_Data_21_08_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_21_08_2025.xlsx: 100%|██████████| 4/4 [00:07<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No active rows found in Mock_Data_21_08_2025.xlsx\n",
      "Processing: C:\\Users\\HP\\Desktop\\PhishingDetection\\models\\model_training\\PhishingUrlData\\Mock_Data_22_08_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_22_08_2025.xlsx: 100%|██████████| 6/6 [00:03<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote 1 active rows to sheet 'Active_Domains' in Mock_Data_22_08_2025.xlsx\n",
      "Processing: C:\\Users\\HP\\Desktop\\PhishingDetection\\models\\model_training\\PhishingUrlData\\Mock_Data_25_08_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_25_08_2025.xlsx: 100%|██████████| 7/7 [00:23<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No active rows found in Mock_Data_25_08_2025.xlsx\n",
      "Processing: C:\\Users\\HP\\Desktop\\PhishingDetection\\models\\model_training\\PhishingUrlData\\Mock_Data_28_08_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_28_08_2025.xlsx: 100%|██████████| 4/4 [00:03<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No active rows found in Mock_Data_28_08_2025.xlsx\n",
      "Processing: C:\\Users\\HP\\Desktop\\PhishingDetection\\models\\model_training\\PhishingUrlData\\Mock_Data_29_08_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_29_08_2025.xlsx: 100%|██████████| 12/12 [00:05<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No active rows found in Mock_Data_29_08_2025.xlsx\n",
      "Processing: C:\\Users\\HP\\Desktop\\PhishingDetection\\models\\model_training\\PhishingUrlData\\Mock_Data_31_07_2025.xlsx\n",
      "  Using column: Identified Phishing/Suspected Domain Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_31_07_2025.xlsx: 100%|██████████| 302/302 [00:44<00:00,  6.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote 20 active rows to sheet 'Active_Domains' in Mock_Data_31_07_2025.xlsx\n",
      "Processing: C:\\Users\\HP\\Desktop\\PhishingDetection\\models\\model_training\\PhishingUrlData\\~$Mock_Data_01_08_2025.xlsx\n",
      "❌ Error reading ~$Mock_Data_01_08_2025.xlsx: [Errno 2] No such file or directory: 'C:\\\\Users\\\\HP\\\\Desktop\\\\PhishingDetection\\\\models\\\\model_training\\\\PhishingUrlData\\\\~$Mock_Data_01_08_2025.xlsx'\n",
      "📄 Summary written to checker_summary.csv\n",
      "✅ Saved 1640 active domains to ActiveDomains_Final.xlsx\n",
      "⚠️ Error processing labels in ~$Mock_Data_01_08_2025.xlsx: [Errno 2] No such file or directory: 'C:\\\\Users\\\\HP\\\\Desktop\\\\PhishingDetection\\\\models\\\\model_training\\\\PhishingUrlData\\\\~$Mock_Data_01_08_2025.xlsx'\n",
      "✅ Categorized outputs saved:\n",
      "   • Active_Phishing.csv (1500)\n",
      "   • Active_Suspected.csv (8)\n",
      "   • Active_Others.csv (0)\n",
      "   • ActiveDomains_ByCategory.xlsx created successfully\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import aiodns\n",
    "import pandas as pd\n",
    "import socket\n",
    "import os\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import ssl\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# CONFIG\n",
    "FOLDER = Path(\"C:\\\\Users\\\\HP\\\\Desktop\\\\PhishingDetection\\\\models\\\\model_training\\\\PhishingUrlData\")\n",
    "EXCEL_GLOB = \"*.xls*\"\n",
    "SHEET_NAME_OUT = \"Active_Domains\"\n",
    "POSSIBLE_COLUMN_NAMES = [\n",
    "    \"Identified Phishing/Suspected Domain Name\",\n",
    "    \"Identified Phishing/Suspected Domain\",\n",
    "    \"Domain\",\n",
    "    \"URL\",\n",
    "    \"Identified Phishing/Suspected Domain Name \",\n",
    "    \"Identified Phishing/Suspected Domain Name  \",\n",
    "    \"Phishing/Suspected Domain Name\",\n",
    "    \"Identified Phishing/Suspected Domain\",\n",
    "    \"Identified Phishing/Suspected Domain Name\"\n",
    "]\n",
    "MAX_CONCURRENT = 40\n",
    "REQUEST_TIMEOUT = 15\n",
    "DNS_TIMEOUT = 10\n",
    "FOLLOW_REDIRECTS = True\n",
    "VERIFY_SSL = True\n",
    "SUMMARY_CSV = \"checker_summary.csv\"\n",
    "\n",
    "# Helper to normalise extracted candidate\n",
    "def normalize_value(v):\n",
    "    if pd.isna(v):\n",
    "        return None\n",
    "    s = str(v).strip()\n",
    "    return s if s else None\n",
    "\n",
    "# DNS resolve (uses aiodns)\n",
    "async def resolve_domain(resolver, domain):\n",
    "    try:\n",
    "        ans = await resolver.gethostbyname(domain, socket.AF_INET)\n",
    "        if ans and ans.addresses:\n",
    "            return True, ans.addresses\n",
    "        return False, []\n",
    "    except Exception:\n",
    "        return False, []\n",
    "\n",
    "# Try HTTP(S)\n",
    "async def try_http(session, candidate):\n",
    "    tried = []\n",
    "    to_try = []\n",
    "    parsed = urlparse(candidate)\n",
    "    if parsed.scheme:\n",
    "        to_try.append(candidate)\n",
    "    else:\n",
    "        to_try.extend([candidate, \"https://\" + candidate, \"http://\" + candidate])\n",
    "\n",
    "    for url in to_try:\n",
    "        if url in tried:\n",
    "            continue\n",
    "        tried.append(url)\n",
    "        try:\n",
    "            timeout = aiohttp.ClientTimeout(total=REQUEST_TIMEOUT)\n",
    "            async with session.head(url, allow_redirects=FOLLOW_REDIRECTS, timeout=timeout) as resp:\n",
    "                if 200 <= resp.status < 400:\n",
    "                    return True, {\"final_url\": str(resp.url), \"status\": resp.status}\n",
    "        except Exception:\n",
    "            try:\n",
    "                async with session.get(url, allow_redirects=FOLLOW_REDIRECTS, timeout=timeout) as resp2:\n",
    "                    if 200 <= resp2.status < 400:\n",
    "                        return True, {\"final_url\": str(resp2.url), \"status\": resp2.status}\n",
    "            except Exception:\n",
    "                continue\n",
    "    return False, {\"error\": \"all attempts failed\"}\n",
    "\n",
    "# Main checker\n",
    "async def check_candidate(session, resolver, raw_value, semaphore):\n",
    "    out = {\"raw_value\": raw_value}\n",
    "    val = normalize_value(raw_value)\n",
    "    out['normalized'] = val\n",
    "    if not val:\n",
    "        out.update({'dns_resolved': False, 'http_active': False, 'http_details': None, 'active': False})\n",
    "        return out\n",
    "\n",
    "    parsed = urlparse(val if \"://\" in val else (\"http://\" + val))\n",
    "    domain = parsed.hostname\n",
    "    dns_ok = False\n",
    "    dns_addrs = []\n",
    "    if domain:\n",
    "        try:\n",
    "            dns_ok, dns_addrs = await resolve_domain(resolver, domain)\n",
    "        except Exception:\n",
    "            dns_ok = False\n",
    "\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            http_ok, http_details = await try_http(session, val)\n",
    "        except Exception as e:\n",
    "            http_ok, http_details = False, {\"error\": str(e)}\n",
    "\n",
    "    out.update({\n",
    "        'dns_resolved': dns_ok,\n",
    "        'dns_addresses': dns_addrs,\n",
    "        'http_active': http_ok,\n",
    "        'http_details': http_details,\n",
    "        'active': http_ok  # HTTP success defines activity\n",
    "    })\n",
    "    return out\n",
    "\n",
    "async def process_file(path: Path, summary_rows):\n",
    "    print(f\"Processing: {path}\")\n",
    "    try:\n",
    "        df_all = pd.read_excel(path, sheet_name=0, engine=\"openpyxl\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error reading {path.name}: {e}\")\n",
    "        return\n",
    "\n",
    "    # find domain column\n",
    "    domain_col = None\n",
    "    for cname in POSSIBLE_COLUMN_NAMES:\n",
    "        if cname in df_all.columns:\n",
    "            domain_col = cname\n",
    "            break\n",
    "    if not domain_col:\n",
    "        for col in df_all.columns:\n",
    "            sample = df_all[col].dropna().astype(str).head(50).tolist()\n",
    "            if any(\".\" in s for s in sample):\n",
    "                domain_col = col\n",
    "                break\n",
    "    if not domain_col:\n",
    "        print(f\" No domain column found in {path.name}, skipping.\")\n",
    "        return\n",
    "    print(f\"  Using column: {domain_col}\")\n",
    "\n",
    "    conn = aiohttp.TCPConnector(ssl=ssl.create_default_context() if VERIFY_SSL else False, limit_per_host=MAX_CONCURRENT)\n",
    "    resolver = aiodns.DNSResolver(timeout=DNS_TIMEOUT)\n",
    "    timeout = aiohttp.ClientTimeout(total=REQUEST_TIMEOUT)\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT)\n",
    "\n",
    "    async with aiohttp.ClientSession(connector=conn, timeout=timeout) as session:\n",
    "        tasks = [check_candidate(session, resolver, row[domain_col], semaphore) for _, row in df_all.iterrows()]\n",
    "        results = []\n",
    "        for coro in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=f\"  Checking {path.name}\"):\n",
    "            res = await coro\n",
    "            results.append(res)\n",
    "\n",
    "    active_records = []\n",
    "    for idx, (i, row) in enumerate(df_all.iterrows()):\n",
    "        res = results[idx]\n",
    "        if res.get('active', False):\n",
    "            rec = row.to_dict()\n",
    "            rec.update({\n",
    "                \"_normalized\": res.get(\"normalized\"),\n",
    "                \"_dns\": res.get(\"dns_resolved\"),\n",
    "                \"_http\": res.get(\"http_active\"),\n",
    "                \"_details\": str(res.get(\"http_details\")),\n",
    "            })\n",
    "            active_records.append(rec)\n",
    "\n",
    "        summary_rows.append({\n",
    "            \"filename\": path.name,\n",
    "            \"row_index\": idx,\n",
    "            \"raw_value\": res.get(\"raw_value\"),\n",
    "            \"normalized\": res.get(\"normalized\"),\n",
    "            \"dns_resolved\": res.get(\"dns_resolved\"),\n",
    "            \"http_active\": res.get(\"http_active\"),\n",
    "            \"active\": res.get(\"active\")\n",
    "        })\n",
    "\n",
    "    if active_records:\n",
    "        out_df = pd.DataFrame(active_records)\n",
    "        with pd.ExcelWriter(path, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "            out_df.to_excel(writer, sheet_name=SHEET_NAME_OUT, index=False)\n",
    "        print(f\" Wrote {len(active_records)} active rows to sheet '{SHEET_NAME_OUT}' in {path.name}\")\n",
    "    else:\n",
    "        print(f\" No active rows found in {path.name}\")\n",
    "\n",
    "# ---------------- MAIN FUNCTION ----------------\n",
    "async def main():\n",
    "    folder = Path(FOLDER)\n",
    "    files = list(folder.glob(EXCEL_GLOB))\n",
    "    if not files:\n",
    "        print(f\"No Excel files found in {folder}\")\n",
    "        return\n",
    "\n",
    "    summary_rows = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            await process_file(f, summary_rows)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {f.name}: {e}\")\n",
    "\n",
    "    if not summary_rows:\n",
    "        print(\" No data collected.\")\n",
    "        return\n",
    "\n",
    "    # Save summary\n",
    "    keys = [\"filename\",\"row_index\",\"raw_value\",\"normalized\",\"dns_resolved\",\"http_active\",\"active\"]\n",
    "    df_summary = pd.DataFrame(summary_rows)\n",
    "    df_summary.to_csv(SUMMARY_CSV, index=False)\n",
    "    print(f\" Summary written to {SUMMARY_CSV}\")\n",
    "\n",
    "    # Filter only active domains\n",
    "    active_df = df_summary[df_summary[\"active\"] == True]\n",
    "    if active_df.empty:\n",
    "        print(\" No active domains found.\")\n",
    "        return\n",
    "\n",
    "    # Save overall active list\n",
    "    active_file = \"ActiveDomains_Final.xlsx\"\n",
    "    active_df.to_excel(active_file, index=False)\n",
    "    print(f\" Saved {len(active_df)} active domains to {active_file}\")\n",
    "\n",
    "    # ---------------- CATEGORIZATION PART ----------------\n",
    "    label_col = \"Phishing/Suspected Domains (i.e. Class Label)\"\n",
    "    phishing_all = []\n",
    "    suspected_all = []\n",
    "    others_all = []\n",
    "\n",
    "    # Iterate again through all Excel files to fetch label column\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = pd.read_excel(f, engine=\"openpyxl\")\n",
    "            if label_col not in df.columns:\n",
    "                continue\n",
    "\n",
    "            domain_col = None\n",
    "            for cname in POSSIBLE_COLUMN_NAMES:\n",
    "                if cname in df.columns:\n",
    "                    domain_col = cname\n",
    "                    break\n",
    "            if not domain_col:\n",
    "                continue\n",
    "\n",
    "            merged = df.merge(active_df, left_on=domain_col, right_on=\"normalized\", how=\"inner\")\n",
    "            if merged.empty:\n",
    "                continue\n",
    "\n",
    "            merged[\"_label_norm\"] = merged[label_col].astype(str).str.lower().str.strip()\n",
    "\n",
    "            phishing_part = merged[merged[\"_label_norm\"].str.contains(\"phish\", na=False)]\n",
    "            suspected_part = merged[merged[\"_label_norm\"].str.contains(\"suspect|suspicious\", na=False)]\n",
    "            other_part = merged[~(\n",
    "                merged.index.isin(phishing_part.index) | merged.index.isin(suspected_part.index)\n",
    "            )]\n",
    "\n",
    "            phishing_all.append(phishing_part)\n",
    "            suspected_all.append(suspected_part)\n",
    "            others_all.append(other_part)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing labels in {f.name}: {e}\")\n",
    "\n",
    "    phishing_df = pd.concat(phishing_all, ignore_index=True) if phishing_all else pd.DataFrame()\n",
    "    suspected_df = pd.concat(suspected_all, ignore_index=True) if suspected_all else pd.DataFrame()\n",
    "    others_df = pd.concat(others_all, ignore_index=True) if others_all else pd.DataFrame()\n",
    "\n",
    "    phishing_df.to_csv(\"Active_Phishing.csv\", index=False)\n",
    "    suspected_df.to_csv(\"Active_Suspected.csv\", index=False)\n",
    "    others_df.to_csv(\"Active_Others.csv\", index=False)\n",
    "\n",
    "    with pd.ExcelWriter(\"ActiveDomains_ByCategory.xlsx\", engine=\"openpyxl\") as writer:\n",
    "        if not phishing_df.empty:\n",
    "            phishing_df.to_excel(writer, sheet_name=\"Phishing\", index=False)\n",
    "        if not suspected_df.empty:\n",
    "            suspected_df.to_excel(writer, sheet_name=\"Suspected\", index=False)\n",
    "        if not others_df.empty:\n",
    "            others_df.to_excel(writer, sheet_name=\"Others\", index=False)\n",
    "\n",
    "    print(\" Categorized outputs saved:\")\n",
    "    print(f\"   • Active_Phishing.csv ({len(phishing_df)})\")\n",
    "    print(f\"   • Active_Suspected.csv ({len(suspected_df)})\")\n",
    "    print(f\"   • Active_Others.csv ({len(others_df)})\")\n",
    "    print(f\"   • ActiveDomains_ByCategory.xlsx created successfully\")\n",
    "\n",
    "# Run main\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "526ea7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 Processing file: Mock_Data_01_08_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_01_08_2025.xlsx: 100%|██████████| 1244/1244 [02:01<00:00, 10.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 687 active URLs written to 'Active_Domains' in Mock_Data_01_08_2025.xlsx\n",
      "\n",
      "📄 Processing file: Mock_Data_03_09_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_03_09_2025.xlsx: 100%|██████████| 1372/1372 [00:41<00:00, 33.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Could not append to Mock_Data_03_09_2025.xlsx, saved to Mock_Data_03_09_2025_Active.xlsx. Error: [Errno 13] Permission denied: 'C:\\\\Users\\\\HP\\\\Desktop\\\\PhishingDetection\\\\models\\\\model_training\\\\PhishingUrlData\\\\Mock_Data_03_09_2025.xlsx'\n",
      "\n",
      "📄 Processing file: Mock_Data_03_09_2025_Active.xlsx\n",
      "⚠️ No domain column found in Mock_Data_03_09_2025_Active.xlsx, skipping.\n",
      "\n",
      "📄 Processing file: Mock_Data_04_08_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_04_08_2025.xlsx: 100%|██████████| 29/29 [00:08<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 15 active URLs written to 'Active_Domains' in Mock_Data_04_08_2025.xlsx\n",
      "\n",
      "📄 Processing file: Mock_Data_05_08_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_05_08_2025.xlsx: 100%|██████████| 1/1 [00:00<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No active rows found in Mock_Data_05_08_2025.xlsx\n",
      "\n",
      "📄 Processing file: Mock_Data_07_08_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_07_08_2025.xlsx: 100%|██████████| 6/6 [00:06<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 6 active URLs written to 'Active_Domains' in Mock_Data_07_08_2025.xlsx\n",
      "\n",
      "📄 Processing file: Mock_Data_12_08_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_12_08_2025.xlsx: 100%|██████████| 7/7 [00:57<00:00,  8.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 3 active URLs written to 'Active_Domains' in Mock_Data_12_08_2025.xlsx\n",
      "\n",
      "📄 Processing file: Mock_Data_12_09_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_12_09_2025.xlsx: 100%|██████████| 277/277 [00:49<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 156 active URLs written to 'Active_Domains' in Mock_Data_12_09_2025.xlsx\n",
      "\n",
      "📄 Processing file: Mock_Data_14_08_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_14_08_2025.xlsx: 100%|██████████| 1/1 [00:01<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 1 active URLs written to 'Active_Domains' in Mock_Data_14_08_2025.xlsx\n",
      "\n",
      "📄 Processing file: Mock_Data_19_08_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_19_08_2025.xlsx: 100%|██████████| 15/15 [00:21<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 7 active URLs written to 'Active_Domains' in Mock_Data_19_08_2025.xlsx\n",
      "\n",
      "📄 Processing file: Mock_Data_21_08_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_21_08_2025.xlsx: 100%|██████████| 4/4 [00:03<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 4 active URLs written to 'Active_Domains' in Mock_Data_21_08_2025.xlsx\n",
      "\n",
      "📄 Processing file: Mock_Data_22_08_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_22_08_2025.xlsx: 100%|██████████| 6/6 [00:41<00:00,  6.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 6 active URLs written to 'Active_Domains' in Mock_Data_22_08_2025.xlsx\n",
      "\n",
      "📄 Processing file: Mock_Data_25_08_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_25_08_2025.xlsx: 100%|██████████| 7/7 [00:22<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2 active URLs written to 'Active_Domains' in Mock_Data_25_08_2025.xlsx\n",
      "\n",
      "📄 Processing file: Mock_Data_28_08_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_28_08_2025.xlsx: 100%|██████████| 4/4 [00:02<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 1 active URLs written to 'Active_Domains' in Mock_Data_28_08_2025.xlsx\n",
      "\n",
      "📄 Processing file: Mock_Data_29_08_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_29_08_2025.xlsx: 100%|██████████| 12/12 [00:00<00:00, 12.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2 active URLs written to 'Active_Domains' in Mock_Data_29_08_2025.xlsx\n",
      "\n",
      "📄 Processing file: Mock_Data_31_07_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_31_07_2025.xlsx: 100%|██████████| 302/302 [00:45<00:00,  6.58it/s]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_22532\\2879773346.py:173: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_summary = pd.concat(summary_rows, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 244 active URLs written to 'Active_Domains' in Mock_Data_31_07_2025.xlsx\n",
      "\n",
      "📄 Processing file: ~$Mock_Data_03_09_2025.xlsx\n",
      "❌ Failed to read C:\\Users\\HP\\Desktop\\PhishingDetection\\models\\model_training\\PhishingUrlData\\~$Mock_Data_03_09_2025.xlsx: [Errno 13] Permission denied: 'C:\\\\Users\\\\HP\\\\Desktop\\\\PhishingDetection\\\\models\\\\model_training\\\\PhishingUrlData\\\\~$Mock_Data_03_09_2025.xlsx'\n",
      "\n",
      "📊 Summary saved to checker_summary.csv\n",
      "✅ ActiveDomains_Final.xlsx created with 2501 entries.\n",
      "\n",
      "🧾 CSVs saved:\n",
      "   • Active_Phishing.csv (2465)\n",
      "   • Active_Suspected.csv (36)\n",
      "   • Active_Others.csv (0)\n",
      "  ✅ Sheet 'Phishing' written with 2465 rows\n",
      "  ✅ Sheet 'Suspected' written with 36 rows\n",
      "\n",
      "📈 Summary Report:\n",
      "   Phishing:  2465 rows\n",
      "   Suspected: 36 rows\n",
      "   Others:    0 rows\n",
      "   Total Active: 2501\n",
      "\n",
      "✅ Categorized Excel created: ActiveDomains_ByCategory.xlsx (auto-split if >1M rows)\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import aiodns\n",
    "import pandas as pd\n",
    "import socket\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import ssl\n",
    "import re\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ===== CONFIG =====\n",
    "FOLDER = Path(\"C:\\\\Users\\\\HP\\\\Desktop\\\\PhishingDetection\\\\models\\\\model_training\\\\PhishingUrlData\")\n",
    "EXCEL_GLOB = \"*.xls*\"\n",
    "SHEET_NAME_OUT = \"Active_Domains\"\n",
    "POSSIBLE_COLUMN_NAMES = [\n",
    "    \"Identified Phishing/Suspected Domain Name\",\n",
    "    \"Identified Phishing/Suspected Domain\",\n",
    "    \"Domain\",\n",
    "    \"URL\"\n",
    "]\n",
    "MAX_CONCURRENT = 40\n",
    "REQUEST_TIMEOUT = 10\n",
    "DNS_TIMEOUT = 5\n",
    "FOLLOW_REDIRECTS = True\n",
    "VERIFY_SSL = True\n",
    "SUMMARY_CSV = \"checker_summary.csv\"\n",
    "LABEL_COL = \"Phishing/Suspected Domains (i.e. Class Label)\"\n",
    "MAX_EXCEL_ROWS = 1_000_000\n",
    "# ==================\n",
    "\n",
    "def canonical_host(value):\n",
    "    \"\"\"Normalize URL/domain into canonical hostname.\"\"\"\n",
    "    if pd.isna(value) or not str(value).strip():\n",
    "        return None\n",
    "    s = str(value).strip()\n",
    "    if \"://\" not in s:\n",
    "        s = \"http://\" + s\n",
    "    try:\n",
    "        host = urlparse(s).hostname\n",
    "        if host:\n",
    "            host = host.lower().rstrip(\".\")\n",
    "            if host.startswith(\"www.\"):\n",
    "                host = host[4:]\n",
    "        return host\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "async def resolve_domain(resolver, domain):\n",
    "    try:\n",
    "        ans = await resolver.gethostbyname(domain, socket.AF_INET)\n",
    "        return bool(ans.addresses), ans.addresses\n",
    "    except Exception:\n",
    "        return False, []\n",
    "\n",
    "async def try_http(session, url):\n",
    "    \"\"\"Try HTTP(S) requests and return status.\"\"\"\n",
    "    candidates = []\n",
    "    p = urlparse(url)\n",
    "    if p.scheme:\n",
    "        candidates.append(url)\n",
    "    else:\n",
    "        candidates += [f\"http://{url}\", f\"https://{url}\"]\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "    for candidate in candidates:\n",
    "        try:\n",
    "            async with session.head(candidate, allow_redirects=FOLLOW_REDIRECTS, timeout=REQUEST_TIMEOUT, headers=headers) as r:\n",
    "                if 200 <= r.status < 400:\n",
    "                    return True, r.status\n",
    "        except Exception:\n",
    "            try:\n",
    "                async with session.get(candidate, allow_redirects=FOLLOW_REDIRECTS, timeout=REQUEST_TIMEOUT, headers=headers) as r2:\n",
    "                    if 200 <= r2.status < 400:\n",
    "                        return True, r2.status\n",
    "            except Exception:\n",
    "                continue\n",
    "    return False, None\n",
    "\n",
    "async def check_domain(session, resolver, domain):\n",
    "    \"\"\"Perform DNS + HTTP checks for one domain.\"\"\"\n",
    "    result = {\"raw_value\": domain, \"host\": canonical_host(domain), \"active\": False}\n",
    "    host = result[\"host\"]\n",
    "    if not host:\n",
    "        return result\n",
    "\n",
    "    try:\n",
    "        dns_ok, _ = await resolve_domain(resolver, host)\n",
    "        http_ok, status = await try_http(session, host)\n",
    "        result.update({\n",
    "            \"dns_resolved\": dns_ok,\n",
    "            \"http_active\": http_ok,\n",
    "            \"http_status\": status,\n",
    "            \"active\": http_ok or dns_ok\n",
    "        })\n",
    "    except Exception as e:\n",
    "        result.update({\"error\": str(e), \"dns_resolved\": False, \"http_active\": False})\n",
    "    return result\n",
    "\n",
    "async def process_file(file, summary_rows):\n",
    "    \"\"\"Process a single Excel file and append active results.\"\"\"\n",
    "    print(f\"\\n📄 Processing file: {file.name}\")\n",
    "    try:\n",
    "        df = pd.read_excel(file)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to read {file}: {e}\")\n",
    "        return\n",
    "\n",
    "    domain_col = next((c for c in POSSIBLE_COLUMN_NAMES if c in df.columns), None)\n",
    "    if not domain_col:\n",
    "        print(f\"⚠️ No domain column found in {file.name}, skipping.\")\n",
    "        return\n",
    "\n",
    "    domains = df[domain_col].dropna().astype(str).tolist()\n",
    "    if not domains:\n",
    "        print(f\"⚠️ No domains found in {file.name}.\")\n",
    "        return\n",
    "\n",
    "    conn = aiohttp.TCPConnector(ssl=ssl.create_default_context() if VERIFY_SSL else False, limit_per_host=MAX_CONCURRENT)\n",
    "    resolver = aiodns.DNSResolver(timeout=DNS_TIMEOUT)\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT)\n",
    "\n",
    "    async with aiohttp.ClientSession(connector=conn) as session:\n",
    "        async def sem_task(d):\n",
    "            async with semaphore:\n",
    "                return await check_domain(session, resolver, d)\n",
    "\n",
    "        tasks = [sem_task(d) for d in domains]\n",
    "        results = []\n",
    "        for fut in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=f\"  Checking {file.name}\"):\n",
    "            try:\n",
    "                res = await fut\n",
    "                results.append(res)\n",
    "            except Exception as e:\n",
    "                results.append({\"raw_value\": None, \"error\": str(e), \"active\": False})\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results[\"filename\"] = file.name\n",
    "    summary_rows.append(df_results)\n",
    "\n",
    "    active_rows = df_results[df_results[\"active\"] == True]\n",
    "    if active_rows.empty:\n",
    "        print(f\"⚠️ No active rows found in {file.name}\")\n",
    "    else:\n",
    "        try:\n",
    "            with pd.ExcelWriter(file, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "                active_rows.to_excel(writer, sheet_name=SHEET_NAME_OUT, index=False)\n",
    "            print(f\"✅ {len(active_rows)} active URLs written to '{SHEET_NAME_OUT}' in {file.name}\")\n",
    "        except Exception as e:\n",
    "            fallback = file.with_name(file.stem + \"_Active.xlsx\")\n",
    "            active_rows.to_excel(fallback, index=False)\n",
    "            print(f\"⚠️ Could not append to {file.name}, saved to {fallback.name}. Error: {e}\")\n",
    "\n",
    "async def main():\n",
    "    folder = Path(FOLDER)\n",
    "    files = list(folder.glob(EXCEL_GLOB))\n",
    "    if not files:\n",
    "        print(\"❌ No Excel files found.\")\n",
    "        return\n",
    "\n",
    "    summary_rows = []\n",
    "    for f in files:\n",
    "        await process_file(f, summary_rows)\n",
    "\n",
    "    if not summary_rows:\n",
    "        print(\"⚠️ No data processed.\")\n",
    "        return\n",
    "\n",
    "    df_summary = pd.concat(summary_rows, ignore_index=True)\n",
    "    df_summary.to_csv(SUMMARY_CSV, index=False)\n",
    "    print(f\"\\n📊 Summary saved to {SUMMARY_CSV}\")\n",
    "\n",
    "    active_df = df_summary[df_summary[\"active\"] == True].copy()\n",
    "    if active_df.empty:\n",
    "        print(\"⚠️ No active domains overall.\")\n",
    "        return\n",
    "\n",
    "    active_df.to_excel(\"ActiveDomains_Final.xlsx\", index=False)\n",
    "    print(f\"✅ ActiveDomains_Final.xlsx created with {len(active_df)} entries.\")\n",
    "\n",
    "    # ---- Categorization (deduplicated) ----\n",
    "    phishing, suspected, others = [], [], []\n",
    "\n",
    "    # ✅ Deduplicate active hosts to prevent merge explosion\n",
    "    active_hosts_df = active_df[['host']].drop_duplicates()\n",
    "\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = pd.read_excel(f)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        if LABEL_COL not in df.columns:\n",
    "            continue\n",
    "\n",
    "        domain_col = next((c for c in POSSIBLE_COLUMN_NAMES if c in df.columns), None)\n",
    "        if not domain_col:\n",
    "            continue\n",
    "\n",
    "        df[\"_host\"] = df[domain_col].apply(canonical_host)\n",
    "        merged = pd.merge(df, active_hosts_df, left_on=\"_host\", right_on=\"host\", how=\"inner\")\n",
    "\n",
    "        if merged.empty:\n",
    "            continue\n",
    "\n",
    "        merged[\"_label_norm\"] = merged[LABEL_COL].astype(str).str.lower().str.strip()\n",
    "\n",
    "        phishing.append(merged[merged[\"_label_norm\"].str.contains(\"phish\", na=False)])\n",
    "        suspected.append(merged[merged[\"_label_norm\"].str.contains(\"suspect|suspicious\", na=False)])\n",
    "        others.append(merged[~merged[\"_label_norm\"].str.contains(\"phish|suspect|suspicious\", na=False)])\n",
    "\n",
    "    phishing_df = pd.concat(phishing, ignore_index=True) if phishing else pd.DataFrame()\n",
    "    suspected_df = pd.concat(suspected, ignore_index=True) if suspected else pd.DataFrame()\n",
    "    others_df = pd.concat(others, ignore_index=True) if others else pd.DataFrame()\n",
    "\n",
    "    phishing_df.to_csv(\"Active_Phishing.csv\", index=False)\n",
    "    suspected_df.to_csv(\"Active_Suspected.csv\", index=False)\n",
    "    others_df.to_csv(\"Active_Others.csv\", index=False)\n",
    "\n",
    "    print(\"\\n🧾 CSVs saved:\")\n",
    "    print(f\"   • Active_Phishing.csv ({len(phishing_df)})\")\n",
    "    print(f\"   • Active_Suspected.csv ({len(suspected_df)})\")\n",
    "    print(f\"   • Active_Others.csv ({len(others_df)})\")\n",
    "\n",
    "    # ---- Excel Export with Auto-Split ----\n",
    "    def write_large_df_to_excel(writer, df, base_name):\n",
    "        if df.empty:\n",
    "            return\n",
    "        total = len(df)\n",
    "        if total <= MAX_EXCEL_ROWS:\n",
    "            df.to_excel(writer, sheet_name=base_name, index=False)\n",
    "            print(f\"  ✅ Sheet '{base_name}' written with {total} rows\")\n",
    "        else:\n",
    "            parts = (total // MAX_EXCEL_ROWS) + 1\n",
    "            for i in range(parts):\n",
    "                start = i * MAX_EXCEL_ROWS\n",
    "                end = min((i + 1) * MAX_EXCEL_ROWS, total)\n",
    "                chunk = df.iloc[start:end]\n",
    "                chunk.to_excel(writer, sheet_name=f\"{base_name}_{i+1}\", index=False)\n",
    "                print(f\"  ➕ Wrote chunk {i+1}/{parts} ({len(chunk)} rows) to '{base_name}_{i+1}'\")\n",
    "\n",
    "    with pd.ExcelWriter(\"ActiveDomains_ByCategory.xlsx\", engine=\"openpyxl\") as w:\n",
    "        write_large_df_to_excel(w, phishing_df, \"Phishing\")\n",
    "        write_large_df_to_excel(w, suspected_df, \"Suspected\")\n",
    "        write_large_df_to_excel(w, others_df, \"Others\")\n",
    "\n",
    "    # ---- Summary Print ----\n",
    "    print(\"\\n📈 Summary Report:\")\n",
    "    print(f\"   Phishing:  {len(phishing_df)} rows\")\n",
    "    print(f\"   Suspected: {len(suspected_df)} rows\")\n",
    "    print(f\"   Others:    {len(others_df)} rows\")\n",
    "    print(f\"   Total Active: {len(active_df)}\")\n",
    "    print(\"\\n✅ Categorized Excel created: ActiveDomains_ByCategory.xlsx (auto-split if >1M rows)\")\n",
    "\n",
    "# ---- Run ----\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83bb213a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 Processing file: Mock_Data_01_08_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_01_08_2025.xlsx: 100%|██████████| 1244/1244 [02:19<00:00,  8.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 686 active URLs written to 'Active_Domains' in Mock_Data_01_08_2025.xlsx\n",
      "   🚨 13 domains verified as likely phishing (score ≥50)\n",
      "\n",
      "📄 Processing file: Mock_Data_03_09_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_03_09_2025.xlsx: 100%|██████████| 1372/1372 [01:29<00:00, 15.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 1367 active URLs written to 'Active_Domains' in Mock_Data_03_09_2025.xlsx\n",
      "   🚨 2 domains verified as likely phishing (score ≥50)\n",
      "\n",
      "📄 Processing file: Mock_Data_03_09_2025_Active.xlsx\n",
      "⚠️ No domain column found in Mock_Data_03_09_2025_Active.xlsx, skipping.\n",
      "\n",
      "📄 Processing file: Mock_Data_04_08_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_04_08_2025.xlsx: 100%|██████████| 29/29 [00:08<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 15 active URLs written to 'Active_Domains' in Mock_Data_04_08_2025.xlsx\n",
      "\n",
      "📄 Processing file: Mock_Data_05_08_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_05_08_2025.xlsx: 100%|██████████| 1/1 [00:00<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No active rows found in Mock_Data_05_08_2025.xlsx\n",
      "\n",
      "📄 Processing file: Mock_Data_07_08_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_07_08_2025.xlsx: 100%|██████████| 6/6 [00:42<00:00,  7.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 6 active URLs written to 'Active_Domains' in Mock_Data_07_08_2025.xlsx\n",
      "\n",
      "📄 Processing file: Mock_Data_12_08_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_12_08_2025.xlsx: 100%|██████████| 7/7 [00:47<00:00,  6.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 3 active URLs written to 'Active_Domains' in Mock_Data_12_08_2025.xlsx\n",
      "\n",
      "📄 Processing file: Mock_Data_12_09_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_12_09_2025.xlsx: 100%|██████████| 277/277 [00:49<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 156 active URLs written to 'Active_Domains' in Mock_Data_12_09_2025.xlsx\n",
      "   🚨 1 domains verified as likely phishing (score ≥50)\n",
      "\n",
      "📄 Processing file: Mock_Data_14_08_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_14_08_2025.xlsx: 100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 1 active URLs written to 'Active_Domains' in Mock_Data_14_08_2025.xlsx\n",
      "\n",
      "📄 Processing file: Mock_Data_19_08_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_19_08_2025.xlsx: 100%|██████████| 15/15 [00:15<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 7 active URLs written to 'Active_Domains' in Mock_Data_19_08_2025.xlsx\n",
      "\n",
      "📄 Processing file: Mock_Data_21_08_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_21_08_2025.xlsx: 100%|██████████| 4/4 [00:06<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 4 active URLs written to 'Active_Domains' in Mock_Data_21_08_2025.xlsx\n",
      "\n",
      "📄 Processing file: Mock_Data_22_08_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_22_08_2025.xlsx: 100%|██████████| 6/6 [00:41<00:00,  6.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 6 active URLs written to 'Active_Domains' in Mock_Data_22_08_2025.xlsx\n",
      "\n",
      "📄 Processing file: Mock_Data_25_08_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_25_08_2025.xlsx: 100%|██████████| 7/7 [00:23<00:00,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2 active URLs written to 'Active_Domains' in Mock_Data_25_08_2025.xlsx\n",
      "\n",
      "📄 Processing file: Mock_Data_28_08_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_28_08_2025.xlsx: 100%|██████████| 4/4 [00:02<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 1 active URLs written to 'Active_Domains' in Mock_Data_28_08_2025.xlsx\n",
      "\n",
      "📄 Processing file: Mock_Data_29_08_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_29_08_2025.xlsx: 100%|██████████| 12/12 [00:00<00:00, 27.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2 active URLs written to 'Active_Domains' in Mock_Data_29_08_2025.xlsx\n",
      "\n",
      "📄 Processing file: Mock_Data_31_07_2025.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking Mock_Data_31_07_2025.xlsx: 100%|██████████| 302/302 [00:45<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 244 active URLs written to 'Active_Domains' in Mock_Data_31_07_2025.xlsx\n",
      "\n",
      "📊 Summary saved to checker_summary.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_22532\\989373228.py:349: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_summary = pd.concat(summary_rows, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ActiveDomains_Final.xlsx created with 2500 entries.\n",
      "\n",
      "🧾 CSVs saved:\n",
      "   • Active_Phishing.csv (1546904)\n",
      "   • Active_Suspected.csv (36)\n",
      "   • Active_Others.csv (0)\n",
      "   🚨 Verified_Phishing_HighConfidence.csv (16) - Score ≥50\n",
      "\n",
      "📈 Phishing Score Distribution:\n",
      "   High Risk (≥70):  1\n",
      "   Medium Risk (50-69): 15\n",
      "   Low Risk (30-49): 42\n",
      "   Minimal Risk (<30): 2442\n",
      "  ➕ Wrote chunk 1/2 (1000000 rows) to 'Phishing_1'\n",
      "  ➕ Wrote chunk 2/2 (546904 rows) to 'Phishing_2'\n",
      "  ✅ Sheet 'Suspected' written with 36 rows\n",
      "  ✅ Sheet 'Verified_HighConf' written with 16 rows\n",
      "\n",
      "📈 Summary Report:\n",
      "   Phishing:  1546904 rows\n",
      "   Suspected: 36 rows\n",
      "   Others:    0 rows\n",
      "   🚨 Verified High Confidence: 16 rows\n",
      "   Total Active: 2500\n",
      "\n",
      "✅ Categorized Excel created: ActiveDomains_ByCategory.xlsx\n",
      "✅ Use 'Verified_Phishing_HighConfidence.csv' for model training!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import aiodns\n",
    "import pandas as pd\n",
    "import socket\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "import csv\n",
    "import ssl\n",
    "import re\n",
    "import nest_asyncio\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ===== CONFIG =====\n",
    "FOLDER = Path(\"C:\\\\Users\\\\HP\\\\Desktop\\\\PhishingDetection\\\\models\\\\model_training\\\\PhishingUrlData\")\n",
    "EXCEL_GLOB = \"*.xls*\"\n",
    "SHEET_NAME_OUT = \"Active_Domains\"\n",
    "POSSIBLE_COLUMN_NAMES = [\n",
    "    \"Identified Phishing/Suspected Domain Name\",\n",
    "    \"Identified Phishing/Suspected Domain\",\n",
    "    \"Domain\",\n",
    "    \"URL\"\n",
    "]\n",
    "MAX_CONCURRENT = 40\n",
    "REQUEST_TIMEOUT = 10\n",
    "DNS_TIMEOUT = 5\n",
    "FOLLOW_REDIRECTS = True\n",
    "VERIFY_SSL = False  # Many phishing sites have invalid certs\n",
    "SUMMARY_CSV = \"checker_summary.csv\"\n",
    "LABEL_COL = \"Phishing/Suspected Domains (i.e. Class Label)\"\n",
    "MAX_EXCEL_ROWS = 1_000_000\n",
    "\n",
    "# ===== PHISHING DETECTION CONFIG =====\n",
    "SUSPICIOUS_KEYWORDS = [\n",
    "    'login', 'signin', 'verify', 'account', 'secure', 'update', \n",
    "    'suspended', 'locked', 'confirm', 'banking', 'paypal', 'amazon',\n",
    "    'password', 'credential', 'validation', 'authentication'\n",
    "]\n",
    "BRAND_NAMES = [\n",
    "    'paypal', 'amazon', 'microsoft', 'apple', 'google', 'facebook',\n",
    "    'instagram', 'twitter', 'netflix', 'bank', 'chase', 'wellsfargo',\n",
    "    'citibank', 'dhl', 'fedex', 'usps', 'irs', 'irs.gov'\n",
    "]\n",
    "# ==================\n",
    "\n",
    "def canonical_host(value):\n",
    "    \"\"\"Normalize URL/domain into canonical hostname.\"\"\"\n",
    "    if pd.isna(value) or not str(value).strip():\n",
    "        return None\n",
    "    s = str(value).strip()\n",
    "    if \"://\" not in s:\n",
    "        s = \"http://\" + s\n",
    "    try:\n",
    "        host = urlparse(s).hostname\n",
    "        if host:\n",
    "            host = host.lower().rstrip(\".\")\n",
    "            if host.startswith(\"www.\"):\n",
    "                host = host[4:]\n",
    "        return host\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def calculate_phishing_score(url, html_content, status_code, ssl_valid):\n",
    "    \"\"\"Calculate phishing likelihood score based on multiple indicators.\"\"\"\n",
    "    score = 0\n",
    "    indicators = []\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    # 1. Check for suspicious URL patterns\n",
    "    if re.search(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', url):  # IP address\n",
    "        score += 25\n",
    "        indicators.append(\"IP_ADDRESS_IN_URL\")\n",
    "    \n",
    "    if url.count('.') > 4:  # Too many subdomains\n",
    "        score += 15\n",
    "        indicators.append(\"EXCESSIVE_SUBDOMAINS\")\n",
    "    \n",
    "    if url.count('-') > 3:  # Excessive hyphens\n",
    "        score += 10\n",
    "        indicators.append(\"EXCESSIVE_HYPHENS\")\n",
    "    \n",
    "    if '@' in url:  # @ symbol in URL\n",
    "        score += 20\n",
    "        indicators.append(\"AT_SYMBOL_IN_URL\")\n",
    "    \n",
    "    # 2. Check for suspicious keywords\n",
    "    keyword_matches = [kw for kw in SUSPICIOUS_KEYWORDS if kw in url_lower]\n",
    "    if keyword_matches:\n",
    "        score += len(keyword_matches) * 5\n",
    "        indicators.append(f\"SUSPICIOUS_KEYWORDS:{','.join(keyword_matches[:3])}\")\n",
    "    \n",
    "    # 3. Check for brand impersonation\n",
    "    brand_matches = [brand for brand in BRAND_NAMES if brand in url_lower]\n",
    "    if brand_matches:\n",
    "        # Check if it's NOT the official domain\n",
    "        parsed = urlparse(url if '://' in url else 'http://' + url)\n",
    "        hostname = parsed.hostname or ''\n",
    "        for brand in brand_matches:\n",
    "            # If brand name is in subdomain/path but not main domain\n",
    "            if brand in hostname and not hostname.endswith(f'{brand}.com'):\n",
    "                score += 20\n",
    "                indicators.append(f\"BRAND_IMPERSONATION:{brand}\")\n",
    "    \n",
    "    # 4. Check URL length\n",
    "    if len(url) > 75:\n",
    "        score += 10\n",
    "        indicators.append(\"LONG_URL\")\n",
    "    \n",
    "    # 5. SSL Certificate issues\n",
    "    if not ssl_valid:\n",
    "        score += 15\n",
    "        indicators.append(\"INVALID_SSL\")\n",
    "    \n",
    "    # 6. Analyze HTML content if available\n",
    "    if html_content:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Check for forms (common in phishing)\n",
    "        forms = soup.find_all('form')\n",
    "        if forms:\n",
    "            score += 10\n",
    "            indicators.append(f\"FORMS_FOUND:{len(forms)}\")\n",
    "            \n",
    "            # Check for password inputs\n",
    "            password_inputs = soup.find_all('input', {'type': 'password'})\n",
    "            if password_inputs:\n",
    "                score += 15\n",
    "                indicators.append(\"PASSWORD_INPUT_FOUND\")\n",
    "        \n",
    "        # Check for hidden iframes (common phishing technique)\n",
    "        iframes = soup.find_all('iframe', style=re.compile(r'display\\s*:\\s*none', re.I))\n",
    "        if iframes:\n",
    "            score += 20\n",
    "            indicators.append(\"HIDDEN_IFRAMES\")\n",
    "        \n",
    "        # Check for external form actions\n",
    "        for form in forms:\n",
    "            action = form.get('action', '')\n",
    "            if action and action.startswith('http') and urlparse(action).hostname != urlparse(url).hostname:\n",
    "                score += 20\n",
    "                indicators.append(\"EXTERNAL_FORM_ACTION\")\n",
    "                break\n",
    "        \n",
    "        # Check page title and content for brand names\n",
    "        title = soup.find('title')\n",
    "        if title:\n",
    "            title_text = title.get_text().lower()\n",
    "            content_brands = [b for b in BRAND_NAMES if b in title_text]\n",
    "            if content_brands and not any(b in url_lower for b in content_brands):\n",
    "                score += 15\n",
    "                indicators.append(f\"BRAND_IN_TITLE_NOT_URL:{content_brands[0]}\")\n",
    "    \n",
    "    # 7. Check for redirects (status codes)\n",
    "    if status_code and status_code in [301, 302, 307, 308]:\n",
    "        score += 10\n",
    "        indicators.append(f\"REDIRECT:{status_code}\")\n",
    "    \n",
    "    return min(score, 100), indicators\n",
    "\n",
    "async def resolve_domain(resolver, domain):\n",
    "    try:\n",
    "        ans = await resolver.gethostbyname(domain, socket.AF_INET)\n",
    "        return bool(ans.addresses), ans.addresses\n",
    "    except Exception:\n",
    "        return False, []\n",
    "\n",
    "async def try_http_advanced(session, url):\n",
    "    \"\"\"Enhanced HTTP check with content analysis.\"\"\"\n",
    "    candidates = []\n",
    "    p = urlparse(url)\n",
    "    if p.scheme:\n",
    "        candidates.append(url)\n",
    "    else:\n",
    "        candidates += [f\"https://{url}\", f\"http://{url}\"]\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\"\n",
    "    }\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        try:\n",
    "            # Try HEAD first\n",
    "            async with session.head(candidate, allow_redirects=FOLLOW_REDIRECTS, \n",
    "                                   timeout=REQUEST_TIMEOUT, headers=headers, ssl=False) as r:\n",
    "                if 200 <= r.status < 400:\n",
    "                    # Get full content with GET\n",
    "                    try:\n",
    "                        async with session.get(candidate, allow_redirects=FOLLOW_REDIRECTS,\n",
    "                                             timeout=REQUEST_TIMEOUT, headers=headers, ssl=False) as r2:\n",
    "                            html = await r2.text()\n",
    "                            ssl_valid = r2.url.scheme == 'https'\n",
    "                            return True, r2.status, html, ssl_valid, str(r2.url)\n",
    "                    except Exception:\n",
    "                        return True, r.status, None, False, candidate\n",
    "        except Exception:\n",
    "            try:\n",
    "                # Direct GET if HEAD fails\n",
    "                async with session.get(candidate, allow_redirects=FOLLOW_REDIRECTS,\n",
    "                                     timeout=REQUEST_TIMEOUT, headers=headers, ssl=False) as r2:\n",
    "                    if 200 <= r2.status < 400:\n",
    "                        html = await r2.text()\n",
    "                        ssl_valid = r2.url.scheme == 'https'\n",
    "                        return True, r2.status, html, ssl_valid, str(r2.url)\n",
    "            except Exception:\n",
    "                continue\n",
    "    \n",
    "    return False, None, None, False, None\n",
    "\n",
    "async def check_domain(session, resolver, domain):\n",
    "    \"\"\"Enhanced domain check with phishing detection.\"\"\"\n",
    "    result = {\n",
    "        \"raw_value\": domain, \n",
    "        \"host\": canonical_host(domain), \n",
    "        \"active\": False,\n",
    "        \"phishing_score\": 0,\n",
    "        \"phishing_indicators\": \"\",\n",
    "        \"verified_phishing\": False,\n",
    "        \"content_hash\": None,\n",
    "        \"final_url\": None\n",
    "    }\n",
    "    \n",
    "    host = result[\"host\"]\n",
    "    if not host:\n",
    "        return result\n",
    "\n",
    "    try:\n",
    "        dns_ok, ip_addresses = await resolve_domain(resolver, host)\n",
    "        http_ok, status, html, ssl_valid, final_url = await try_http_advanced(session, host)\n",
    "        \n",
    "        result.update({\n",
    "            \"dns_resolved\": dns_ok,\n",
    "            \"http_active\": http_ok,\n",
    "            \"http_status\": status,\n",
    "            \"active\": http_ok or dns_ok,\n",
    "            \"ip_addresses\": ','.join(ip_addresses) if ip_addresses else None,\n",
    "            \"ssl_valid\": ssl_valid,\n",
    "            \"final_url\": final_url\n",
    "        })\n",
    "        \n",
    "        if http_ok and html:\n",
    "            # Calculate phishing score\n",
    "            score, indicators = calculate_phishing_score(\n",
    "                final_url or host, html, status, ssl_valid\n",
    "            )\n",
    "            result[\"phishing_score\"] = score\n",
    "            result[\"phishing_indicators\"] = '|'.join(indicators)\n",
    "            result[\"verified_phishing\"] = score >= 50  # Threshold for likely phishing\n",
    "            result[\"content_hash\"] = hashlib.md5(html.encode()).hexdigest()[:16]\n",
    "        \n",
    "    except Exception as e:\n",
    "        result.update({\n",
    "            \"error\": str(e), \n",
    "            \"dns_resolved\": False, \n",
    "            \"http_active\": False\n",
    "        })\n",
    "    \n",
    "    return result\n",
    "\n",
    "async def process_file(file, summary_rows):\n",
    "    \"\"\"Process a single Excel file and append active results.\"\"\"\n",
    "    print(f\"\\n📄 Processing file: {file.name}\")\n",
    "    try:\n",
    "        df = pd.read_excel(file)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to read {file}: {e}\")\n",
    "        return\n",
    "\n",
    "    domain_col = next((c for c in POSSIBLE_COLUMN_NAMES if c in df.columns), None)\n",
    "    if not domain_col:\n",
    "        print(f\"⚠️ No domain column found in {file.name}, skipping.\")\n",
    "        return\n",
    "\n",
    "    domains = df[domain_col].dropna().astype(str).tolist()\n",
    "    if not domains:\n",
    "        print(f\"⚠️ No domains found in {file.name}.\")\n",
    "        return\n",
    "\n",
    "    conn = aiohttp.TCPConnector(ssl=False, limit_per_host=MAX_CONCURRENT)\n",
    "    resolver = aiodns.DNSResolver(timeout=DNS_TIMEOUT)\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT)\n",
    "\n",
    "    async with aiohttp.ClientSession(connector=conn) as session:\n",
    "        async def sem_task(d):\n",
    "            async with semaphore:\n",
    "                return await check_domain(session, resolver, d)\n",
    "\n",
    "        tasks = [sem_task(d) for d in domains]\n",
    "        results = []\n",
    "        for fut in tqdm(asyncio.as_completed(tasks), total=len(tasks), \n",
    "                       desc=f\"  Checking {file.name}\"):\n",
    "            try:\n",
    "                res = await fut\n",
    "                results.append(res)\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    \"raw_value\": None, \n",
    "                    \"error\": str(e), \n",
    "                    \"active\": False,\n",
    "                    \"phishing_score\": 0\n",
    "                })\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results[\"filename\"] = file.name\n",
    "    df_results[\"check_timestamp\"] = datetime.now().isoformat()\n",
    "    summary_rows.append(df_results)\n",
    "\n",
    "    active_rows = df_results[df_results[\"active\"] == True]\n",
    "    if active_rows.empty:\n",
    "        print(f\"⚠️ No active rows found in {file.name}\")\n",
    "    else:\n",
    "        try:\n",
    "            with pd.ExcelWriter(file, engine=\"openpyxl\", mode=\"a\", \n",
    "                              if_sheet_exists=\"replace\") as writer:\n",
    "                active_rows.to_excel(writer, sheet_name=SHEET_NAME_OUT, index=False)\n",
    "            print(f\"✅ {len(active_rows)} active URLs written to '{SHEET_NAME_OUT}' in {file.name}\")\n",
    "            \n",
    "            # Print phishing stats\n",
    "            verified = len(active_rows[active_rows[\"verified_phishing\"] == True])\n",
    "            if verified > 0:\n",
    "                print(f\"   🚨 {verified} domains verified as likely phishing (score ≥50)\")\n",
    "        except Exception as e:\n",
    "            fallback = file.with_name(file.stem + \"_Active.xlsx\")\n",
    "            active_rows.to_excel(fallback, index=False)\n",
    "            print(f\"⚠️ Could not append to {file.name}, saved to {fallback.name}. Error: {e}\")\n",
    "\n",
    "async def main():\n",
    "    folder = Path(FOLDER)\n",
    "    files = list(folder.glob(EXCEL_GLOB))\n",
    "    if not files:\n",
    "        print(\"❌ No Excel files found.\")\n",
    "        return\n",
    "\n",
    "    summary_rows = []\n",
    "    for f in files:\n",
    "        await process_file(f, summary_rows)\n",
    "\n",
    "    if not summary_rows:\n",
    "        print(\"⚠️ No data processed.\")\n",
    "        return\n",
    "\n",
    "    df_summary = pd.concat(summary_rows, ignore_index=True)\n",
    "    df_summary.to_csv(SUMMARY_CSV, index=False)\n",
    "    print(f\"\\n📊 Summary saved to {SUMMARY_CSV}\")\n",
    "\n",
    "    active_df = df_summary[df_summary[\"active\"] == True].copy()\n",
    "    if active_df.empty:\n",
    "        print(\"⚠️ No active domains overall.\")\n",
    "        return\n",
    "\n",
    "    active_df.to_excel(\"ActiveDomains_Final.xlsx\", index=False)\n",
    "    print(f\"✅ ActiveDomains_Final.xlsx created with {len(active_df)} entries.\")\n",
    "\n",
    "    # ---- Enhanced Categorization with Phishing Verification ----\n",
    "    phishing, suspected, others, verified_phishing = [], [], [], []\n",
    "\n",
    "    active_hosts_df = active_df[['host']].drop_duplicates()\n",
    "\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = pd.read_excel(f)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        if LABEL_COL not in df.columns:\n",
    "            continue\n",
    "\n",
    "        domain_col = next((c for c in POSSIBLE_COLUMN_NAMES if c in df.columns), None)\n",
    "        if not domain_col:\n",
    "            continue\n",
    "\n",
    "        df[\"_host\"] = df[domain_col].apply(canonical_host)\n",
    "        merged = pd.merge(df, active_df, left_on=\"_host\", right_on=\"host\", how=\"inner\")\n",
    "\n",
    "        if merged.empty:\n",
    "            continue\n",
    "\n",
    "        merged[\"_label_norm\"] = merged[LABEL_COL].astype(str).str.lower().str.strip()\n",
    "\n",
    "        phishing.append(merged[merged[\"_label_norm\"].str.contains(\"phish\", na=False)])\n",
    "        suspected.append(merged[merged[\"_label_norm\"].str.contains(\"suspect|suspicious\", na=False)])\n",
    "        others.append(merged[~merged[\"_label_norm\"].str.contains(\"phish|suspect|suspicious\", na=False)])\n",
    "\n",
    "    phishing_df = pd.concat(phishing, ignore_index=True) if phishing else pd.DataFrame()\n",
    "    suspected_df = pd.concat(suspected, ignore_index=True) if suspected else pd.DataFrame()\n",
    "    others_df = pd.concat(others, ignore_index=True) if others else pd.DataFrame()\n",
    "\n",
    "    # ---- NEW: Verified Phishing (high-confidence based on analysis) ----\n",
    "    verified_phishing_df = active_df[active_df[\"verified_phishing\"] == True].copy()\n",
    "\n",
    "    phishing_df.to_csv(\"Active_Phishing.csv\", index=False)\n",
    "    suspected_df.to_csv(\"Active_Suspected.csv\", index=False)\n",
    "    others_df.to_csv(\"Active_Others.csv\", index=False)\n",
    "    verified_phishing_df.to_csv(\"Verified_Phishing_HighConfidence.csv\", index=False)\n",
    "\n",
    "    print(\"\\n🧾 CSVs saved:\")\n",
    "    print(f\"   • Active_Phishing.csv ({len(phishing_df)})\")\n",
    "    print(f\"   • Active_Suspected.csv ({len(suspected_df)})\")\n",
    "    print(f\"   • Active_Others.csv ({len(others_df)})\")\n",
    "    print(f\"   🚨 Verified_Phishing_HighConfidence.csv ({len(verified_phishing_df)}) - Score ≥50\")\n",
    "\n",
    "    # ---- Phishing Score Analysis ----\n",
    "    if not active_df.empty:\n",
    "        print(\"\\n📈 Phishing Score Distribution:\")\n",
    "        print(f\"   High Risk (≥70):  {len(active_df[active_df['phishing_score'] >= 70])}\")\n",
    "        print(f\"   Medium Risk (50-69): {len(active_df[(active_df['phishing_score'] >= 50) & (active_df['phishing_score'] < 70)])}\")\n",
    "        print(f\"   Low Risk (30-49): {len(active_df[(active_df['phishing_score'] >= 30) & (active_df['phishing_score'] < 50)])}\")\n",
    "        print(f\"   Minimal Risk (<30): {len(active_df[active_df['phishing_score'] < 30])}\")\n",
    "\n",
    "    # ---- Excel Export with Auto-Split ----\n",
    "    def write_large_df_to_excel(writer, df, base_name):\n",
    "        if df.empty:\n",
    "            return\n",
    "        total = len(df)\n",
    "        if total <= MAX_EXCEL_ROWS:\n",
    "            df.to_excel(writer, sheet_name=base_name, index=False)\n",
    "            print(f\"  ✅ Sheet '{base_name}' written with {total} rows\")\n",
    "        else:\n",
    "            parts = (total // MAX_EXCEL_ROWS) + 1\n",
    "            for i in range(parts):\n",
    "                start = i * MAX_EXCEL_ROWS\n",
    "                end = min((i + 1) * MAX_EXCEL_ROWS, total)\n",
    "                chunk = df.iloc[start:end]\n",
    "                chunk.to_excel(writer, sheet_name=f\"{base_name}_{i+1}\", index=False)\n",
    "                print(f\"  ➕ Wrote chunk {i+1}/{parts} ({len(chunk)} rows) to '{base_name}_{i+1}'\")\n",
    "\n",
    "    with pd.ExcelWriter(\"ActiveDomains_ByCategory.xlsx\", engine=\"openpyxl\") as w:\n",
    "        write_large_df_to_excel(w, phishing_df, \"Phishing\")\n",
    "        write_large_df_to_excel(w, suspected_df, \"Suspected\")\n",
    "        write_large_df_to_excel(w, others_df, \"Others\")\n",
    "        write_large_df_to_excel(w, verified_phishing_df, \"Verified_HighConf\")\n",
    "\n",
    "    # ---- Summary Print ----\n",
    "    print(\"\\n📈 Summary Report:\")\n",
    "    print(f\"   Phishing:  {len(phishing_df)} rows\")\n",
    "    print(f\"   Suspected: {len(suspected_df)} rows\")\n",
    "    print(f\"   Others:    {len(others_df)} rows\")\n",
    "    print(f\"   🚨 Verified High Confidence: {len(verified_phishing_df)} rows\")\n",
    "    print(f\"   Total Active: {len(active_df)}\")\n",
    "    print(\"\\n✅ Categorized Excel created: ActiveDomains_ByCategory.xlsx\")\n",
    "    print(\"✅ Use 'Verified_Phishing_HighConfidence.csv' for model training!\")\n",
    "\n",
    "# ---- Run ----\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d6d6ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1546904\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\HP\\\\Desktop\\\\PhishingDetection\\\\models\\\\preprocessing\\\\codes\\\\Active_Phishing.csv\")\n",
    "df.head()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9bde725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\HP\\\\Desktop\\\\PhishingDetection\\\\models\\\\preprocessing\\\\codes\\\\Active_Suspected.csv\")\n",
    "df.head()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00a539bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2501\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"C:\\\\Users\\\\HP\\\\Desktop\\\\PhishingDetection\\\\models\\\\preprocessing\\\\codes\\\\ActiveDomains_final.xlsx\")\n",
    "df.head()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d7d2376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3287\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\HP\\\\Desktop\\\\PhishingDetection\\\\models\\\\preprocessing\\\\codes\\\\checker_summary.csv\")\n",
    "df.head()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc181dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phishing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
